# takens-embedding-transformer
This repository implements a transformer architecture in which attention is replaced by explicit phase-space embedding via Takens’ delay coordinate reconstruction.

**Repository:** `KevinHaylett/takens-embedding-transformer`  
**Author:** Kevin R. Haylett  
**Licence:** Mozilla Public License 2.0 (MPL-2.0)  
**Implementation:** This code implements the architecture described in ‘Introducing the Takens-Based Transformer’ (Haylett, 2025)

---

## Overview

This repository contains a **proof-of-concept implementation** of a language model architecture in which conventional attention mechanisms are replaced by **explicit phase-space embedding** derived from **Takens’ delay-coordinate reconstruction theorem**.

Rather than treating tokens as static vectors linked by attention weights, this approach models language as a **trajectory in a reconstructed dynamical state space**, where semantic structure emerges from the geometry of sequential embeddings.

The purpose of this repository is to **demonstrate the viability and clarity of the approach**, not to provide a production-ready or benchmark-optimised system.

---

## Conceptual Motivation

Modern transformer architectures rely on attention mechanisms that measure pairwise relationships between token embeddings. While effective, this framing obscures a deeper structural interpretation:

> Sequence modelling in transformers is mathematically equivalent to **phase-space reconstruction from delayed observations**.

Takens’ theorem shows that the dynamics of a system can be reconstructed from a sequence of observations using delay-coordinate embeddings. Applied to language, token sequences may be treated as observable signals generated by an underlying semantic dynamical system.

This implementation makes that equivalence **explicit** by constructing delay embeddings directly and operating on the resulting phase-space representation, rather than implicitly approximating it through attention.

---

## Key Characteristics

- **No attention mechanism**
- **Explicit delay-coordinate embeddings**
- **Sequence-as-trajectory interpretation**
- **Small, interpretable, and efficient models**
- Designed for **conceptual clarity and experimentation**

---

## Repository Structure

The repository contains:

- Core modules implementing delay-coordinate embedding
- A minimal transformer-style architecture operating on embedded trajectories
- Training and inference scripts for small demonstrator models
- Example datasets used to validate the approach

The code is intentionally kept compact and readable to support inspection and modification.

---

## Scope and Intent

This repository is intended to:

- Demonstrate that **phase-space embedding can replace attention** in language models
- Provide a **concrete reference implementation** for research discussion
- Support experimentation with small and domain-specific models
- Serve as a foundation for further theoretical and practical exploration

It is **not** intended to:

- Compete directly with large-scale production LLMs
- Optimise benchmark scores
- Serve as a drop-in replacement for standard transformer libraries

---

## Licence

This code is released under the **Mozilla Public License 2.0 (MPL-2.0)**.

- Modifications to individual source files must remain open
- The code may be incorporated into larger systems
- The licence balances openness with traceability of the core implementation

See the `LICENSE` file in the repository root for full details.

---

## Sharing and Citation

If you build upon, extend, or reference this work, academic citation or explicit acknowledgement of the underlying conceptual framework is strongly encouraged.

### To cite the paper

If you use, implement, or build upon the Takens-Based Transformer, please cite the canonical paper:

Kevin R. Haylett, Introducing the Takens-Based Transformer, December 2025.
Available at: https://finitemechanics.com/papers/takens-transformer.pdf

### To cite the code repository

Haylett, K. R. (2025).  
*Takens Embedding Transformer*.  
GitHub repository:  
https://github.com/KevinHaylett/takens-embedding-transformer

---

## Further Information

- https://www.finitemechanics.com  
- https://kevinhaylett.substack.com  
- https://www.geofinitism.com  

---

## Contact

Kevin R. Haylett  
Email: kevin.haylett@gmail.com
